{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/gdrive')\n",
    "\n",
    "# Specify the directory path where `assignemnt3.ipynb` exists.\n",
    "# For example, if you saved `assignment3.ipynb` in `/gdrive/My Drive/cs376/assignment3` directory,\n",
    "# then set root = '/gdrive/My Drive/CS376-2021F/HW3'\n",
    "root = '/gdrive/My Drive/CycleGAN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL.Image import Image\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, datasets, utils\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import Discriminator, Generator\n",
    "from dataset import ImageDataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "torch.manual_seed(470)\n",
    "torch.cuda.manual_seed(470)\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "ROOT_DIR = os.path.dirname(os.path.abspath(__file__))\n",
    "LOG_DIR = os.path.join(ROOT_DIR , \"logs/\" + now.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "LOG_ITER = 100\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "BATCHSIZE = 4\n",
    "LEARNING_RATE = 0.002\n",
    "MAX_EPOCH = 100\n",
    "\n",
    "LAMBDA = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from random import shuffle\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, path_X, path_Y, transform = None):\n",
    "        self.path_X = glob.glob(os.path.join(path_X, '*.jpg'))\n",
    "        self.path_Y = glob.glob(os.path.join(path_Y, '*.jpg'))\n",
    "        shuffle(self.path_X)\n",
    "        shuffle(self.path_Y)\n",
    "        self.transform = transform\n",
    "        self.length = max(len(self.path_X), len(self.path_Y))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_X = Image.open(self.path_X[index % len(self.path_X)])\n",
    "        img_Y = Image.open(self.path_Y[index % len(self.path_Y)])\n",
    "        \n",
    "        if self.transform:\n",
    "            img_X = self.transform(img_X)\n",
    "            img_Y = self.transform(img_Y)\n",
    "        \n",
    "        return img_X, img_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Data Pipeline\n",
    "data_dir_X = os.path.join(Path(ROOT_DIR).parent, 'dataset', 'photo_jpg')\n",
    "data_dir_Y = os.path.join(Path(ROOT_DIR).parent, 'dataset', 'monet_jpg')\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "#Helper Functions\n",
    "def imshow(img):\n",
    "    img = img.numpy()\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show():\n",
    "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
    "        imshow(inputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        conv_block = [nn.ReflectionPad2d(1),\n",
    "                      nn.Conv2d(in_features, in_features, 3),\n",
    "                      nn.InstanceNorm2d(in_features),\n",
    "                      nn.ReLU(inplace=True),\n",
    "                      nn.ReflectionPad2d(1),\n",
    "                      nn.Conv2d(in_features, in_features, 3),\n",
    "                      nn.InstanceNorm2d(in_features)]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.features = self.create_layers()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x\n",
    "\n",
    "    def create_layers(self):\n",
    "        infos = ['c7s1-64', 'd128', 'd256', 'R256', 'R256', 'R256', 'R256',\n",
    "                 'R256', 'R256', 'R256', 'R256', 'R256', 'u128', 'u64', 'c7s1-3']\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in infos:\n",
    "            if x.startswith('c7s1-'):\n",
    "                layers += [nn.ReflectionPad2d(3),\n",
    "                           nn.Conv2d(in_channels, int(x[5:]), kernel_size=7),\n",
    "                           nn.InstanceNorm2d(int(x[5:])),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = int(x[5:])\n",
    "\n",
    "            elif x.startswith('d'):\n",
    "                layers += [nn.Conv2d(in_channels, int(x[1:]), kernel_size=3, stride=2, padding=1),\n",
    "                           nn.InstanceNorm2d(int(x[1:])),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = int(x[1:])\n",
    "\n",
    "            elif x.startswith('R'):\n",
    "                layers += [ResidualBlock(in_channels)]\n",
    "\n",
    "            elif x.startswith('u'):\n",
    "                layers += [nn.ConvTranspose2d(in_channels, int(x[1:]), kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                           nn.InstanceNorm2d(int(x[1:])),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = int(x[1:])\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.features = self.create_layers()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)\n",
    "\n",
    "    def create_layers(self):\n",
    "        infos = [64, 128, 256, 512]\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in infos:\n",
    "            if x == 64:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=4, stride=2),\n",
    "                           nn.InstanceNorm2d(x), \n",
    "                           nn.LeakyReLU(0.2, inplace=True)]\n",
    "                in_channels = x\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=4, stride=2), \n",
    "                           nn.LeakyReLU(0.2, inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.Conv2d(in_channels, 1, 4, padding=1)]\n",
    "        return nn.Sequential(*layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageDataset(data_dir_X, data_dir_Y, transform=transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCHSIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "Generator_XY = Generator()\n",
    "Discriminator_X = Discriminator()\n",
    "\n",
    "Generator_YX = Generator()\n",
    "Discriminator_Y = Discriminator()\n",
    "\n",
    "optimizer_generator = optim.Adam(Generator_XY.parameters(), lr=LEARNING_RATE)\n",
    "optimizer_discriminator_X = optim.Adam(Discriminator_X.parameters(), lr=LEARNING_RATE)\n",
    "optimizer_discriminator_Y = optim.Adam(Discriminator_Y.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "img_size = torch.empty(256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(LOG_DIR)\n",
    "iteration = 0\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    Generator_XY.train()\n",
    "    Generator_YX.train()\n",
    "    Discriminator_X.train()\n",
    "    Discriminator_Y.train()\n",
    "    for input_X, input_Y in train_dataloader:\n",
    "        iteration += 1\n",
    "        input_X = input_X.to(DEVICE)\n",
    "        input_Y = input_Y.to(DEVICE)\n",
    "\n",
    "        X_to_Y = Generator_XY(input_X)\n",
    "        Y_to_X = Generator_YX(input_Y)\n",
    "\n",
    "        # Adversarial Loss\n",
    "        MSELoss = torch.nn.MSELoss()\n",
    "        result_XYY = Discriminator_Y(X_to_Y)\n",
    "        result_YY = Discriminator_Y(input_Y)\n",
    "        result_YXX = Discriminator_X(Y_to_X)\n",
    "        result_XX = Discriminator_X(input_X)\n",
    "\n",
    "        loss_GAN_G = (MSELoss(result_XYY, torch.ones_like(result_XYY)) + MSELoss(result_YXX, torch.ones_like(result_YXX))) / 2\n",
    "\n",
    "        loss_GAN_DY = MSELoss(result_YY, torch.ones_like(result_YY)) + MSELoss(result_XYY, torch.zeros_like(result_XYY))\n",
    "        loss_GAN_DX = MSELoss(result_XX, torch.ones_like(result_XX)) + MSELoss(result_YXX, torch.zeros_like(result_YXX))\n",
    "\n",
    "\n",
    "        # Cycle Consistency Loss\n",
    "        L1Norm = torch.nn.L1Loss()\n",
    "        loss_cyc = (L1Norm(Generator_YX(Y_to_X), input_X) + L1Norm(Generator_XY(Y_to_X), input_Y))*LAMBDA\n",
    "\n",
    "        # Identity Loss\n",
    "        loss_identity = (L1Norm(X_to_Y, input_Y) + L1Norm(Y_to_X, input_X))*0.5*LAMBDA\n",
    "\n",
    "        loss_G = loss_GAN_G + loss_cyc + loss_identity\n",
    "        loss_DX = loss_GAN_DX\n",
    "        loss_DY = loss_GAN_DY\n",
    "\n",
    "        optimizer_discriminator_X.zero_grad()\n",
    "        loss_DX.backward(retain_graph=True)\n",
    "        optimizer_discriminator_X.step()\n",
    "\n",
    "        optimizer_discriminator_Y.zero_grad()\n",
    "        loss_DY.backward(retain_graph=True)\n",
    "        optimizer_discriminator_Y.step()\n",
    "\n",
    "        optimizer_generator.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optimizer_generator.step()\n",
    "        \n",
    "        loss = loss_G.item() + loss_DX.item() + loss_DY.item()\n",
    "\n",
    "        if iteration % 20 == 0 and writer is not None:\n",
    "            writer.add_scalar('train_loss', loss, iteration)\n",
    "            print('[epoch: {}, iteration: {}] train loss : {:4f}'.format(epoch+1, iteration, loss))\n",
    "            \n",
    "    print('[epoch: {}] train loss : {:4f}'.format(epoch+1, loss))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
