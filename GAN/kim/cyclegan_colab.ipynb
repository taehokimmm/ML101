{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taehokimmm/ML101/blob/main/GAN/kim/cyclegan_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jMC_QefoT4h4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3a7346a-4cfc-417a-ed97-93bfe80e5f9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "root = '/gdrive/My Drive/CycleGAN'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "x2N54k-aT4h6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from PIL.Image import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as FF\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import itertools\n",
        "from torchvision.utils import save_image, make_grid\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "torch.manual_seed(470)\n",
        "torch.cuda.manual_seed(470)\n",
        "\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "now = datetime.now()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LakGDg6jT4h7"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "ROOT_DIR = root\n",
        "LOG_DIR = os.path.join(ROOT_DIR, \"logs\", now.strftime(\"%Y%m%d-%H%M%S\"))\n",
        "LOG_ITER = 50\n",
        "CKPT_DIR = os.path.join(root, 'checkpoints')\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCHSIZE = 10\n",
        "LEARNING_RATE = 0.0002\n",
        "MAX_EPOCH = 100\n",
        "\n",
        "LAMBDA = 10\n",
        "\n",
        "if not os.path.exists(CKPT_DIR):\n",
        "  os.makedirs(CKPT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "atP5ttOrT4h7"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, path_X, path_Y=None, transform=None, test=False):\n",
        "        self.path_X = glob.glob(os.path.join(path_X, '*.jpg'))\n",
        "        self.test = test\n",
        "        if test == False:\n",
        "            self.path_Y = glob.glob(os.path.join(path_Y, '*.jpg'))\n",
        "            self.length = max(len(self.path_X), len(self.path_Y))\n",
        "        else:\n",
        "            self.length = len(self.path_X)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_X = Image.open(self.path_X[index % len(self.path_X)]).convert('RGB')\n",
        "        if self.test == False:\n",
        "            img_Y = Image.open(self.path_Y[index % len(self.path_Y)]).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            img_X = self.transform(img_X)\n",
        "            if self.test == False:\n",
        "                img_Y = self.transform(img_Y)\n",
        "\n",
        "        if self.test == False:\n",
        "            return img_X, img_Y\n",
        "        return img_X\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uNtT2XbtT4h8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "485a37f3-de64-437a-da43-3983dd54a9c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7038\n",
            "300\n"
          ]
        }
      ],
      "source": [
        "# Construct Data Pipeline\n",
        "data_dir_X = os.path.join(ROOT_DIR, 'dataset', 'photo_jpg')\n",
        "data_dir_Y = os.path.join(ROOT_DIR, 'dataset', 'monet_jpg')\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "#Helper Functions\n",
        "def initialize_weight(m):\n",
        "    if isinstance(m, torch.nn.Conv2d):\n",
        "        nn.init.normal_(m.weight.data, 0, 0.02)\n",
        "\n",
        "def imshow(img):\n",
        "    img = img.numpy()\n",
        "    plt.imshow(np.transpose(img, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def show():\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_dataloader):\n",
        "        imshow(inputs[0])\n",
        "\n",
        "\n",
        "def show(imgs):\n",
        "    if not isinstance(imgs, list):\n",
        "        imgs = [imgs]\n",
        "    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
        "    for i, img in enumerate(imgs):\n",
        "        img = img.detach()\n",
        "        img = FF.to_pil_image(img)\n",
        "        axs[0, i].imshow(np.asarray(img))\n",
        "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "print(len(glob.glob(os.path.join(data_dir_X, '*.jpg'))))\n",
        "print(len(glob.glob(os.path.join(data_dir_Y, '*.jpg'))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "IqzrJdMQT4h8"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        conv_block = [nn.ReflectionPad2d(1),\n",
        "                      nn.Conv2d(in_features, in_features, 3),\n",
        "                      nn.InstanceNorm2d(in_features),\n",
        "                      nn.ReLU(inplace=True),\n",
        "                      nn.ReflectionPad2d(1),\n",
        "                      nn.Conv2d(in_features, in_features, 3),\n",
        "                      nn.InstanceNorm2d(in_features)]\n",
        "\n",
        "        self.conv_block = nn.Sequential(*conv_block)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.conv_block(x)\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.features = self.create_layers()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x)\n",
        "\n",
        "    def create_layers(self):\n",
        "        infos = ['c7s1-64', 'd128', 'd256', 'R256', 'R256', 'R256', 'R256',\n",
        "                 'R256', 'R256', 'R256', 'R256', 'R256', 'u128', 'u64', 'c7s1-3']\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in infos:\n",
        "            if x.startswith('c7s1-64'):\n",
        "                layers += [nn.ReflectionPad2d(3),\n",
        "                           nn.Conv2d(in_channels, int(x[5:]), kernel_size=7),\n",
        "                           nn.InstanceNorm2d(int(x[5:])),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = int(x[5:])\n",
        "\n",
        "            elif x.startswith('d'):\n",
        "                layers += [nn.Conv2d(in_channels, int(x[1:]), kernel_size=3, stride=2, padding=1),\n",
        "                           nn.InstanceNorm2d(int(x[1:])),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = int(x[1:])\n",
        "\n",
        "            elif x.startswith('R'):\n",
        "                layers += [ResidualBlock(in_channels)]\n",
        "\n",
        "            elif x.startswith('u'):\n",
        "                layers += [nn.ConvTranspose2d(in_channels, int(x[1:]), kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "                           nn.InstanceNorm2d(int(x[1:])),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = int(x[1:])\n",
        "            elif x.startswith('c7s1-3'):\n",
        "                layers+=[nn.ReflectionPad2d(3),\n",
        "                      nn.Conv2d(in_channels, 3, 7),\n",
        "                      nn.Tanh()]\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.features = self.create_layers()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.features(x)\n",
        "\n",
        "    def create_layers(self):\n",
        "        infos = [64, 128, 256, 512]\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in infos:\n",
        "            if x != 64:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=4, stride=2),\n",
        "                           nn.InstanceNorm2d(x), \n",
        "                           nn.LeakyReLU(0.2, inplace=True)]\n",
        "                in_channels = x\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=4, stride=2), \n",
        "                           nn.LeakyReLU(0.2, inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.Conv2d(in_channels, 1, 4, padding=1)]\n",
        "        return nn.Sequential(*layers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FdGetmnoT4h9"
      },
      "outputs": [],
      "source": [
        "train_dataset = ImageDataset(data_dir_X, data_dir_Y, transform=transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=BATCHSIZE, shuffle=True, num_workers=2)\n",
        "\n",
        "Generator_XY = Generator().to(DEVICE).apply(initialize_weight)\n",
        "Discriminator_X = Discriminator().to(DEVICE).apply(initialize_weight)\n",
        "\n",
        "Generator_YX = Generator().to(DEVICE).apply(initialize_weight)\n",
        "Discriminator_Y = Discriminator().to(DEVICE).apply(initialize_weight)\n",
        "\n",
        "optimizer_generator = optim.Adam(itertools.chain(\n",
        "    Generator_XY.parameters(), Generator_YX.parameters()), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "optimizer_discriminator_X = optim.Adam(\n",
        "    Discriminator_X.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "optimizer_discriminator_Y = optim.Adam(\n",
        "    Discriminator_Y.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
        "\n",
        "def schedule_lambda(epoch): return (1 -\n",
        "                                    (epoch - MAX_EPOCH/2 + np.abs(epoch - MAX_EPOCH/2))/(MAX_EPOCH))\n",
        "\n",
        "scheduler_G = optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_generator, lr_lambda=schedule_lambda)\n",
        "scheduler_DX = optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_discriminator_X, lr_lambda=schedule_lambda)\n",
        "scheduler_DY = optim.lr_scheduler.LambdaLR(\n",
        "    optimizer_discriminator_Y, lr_lambda=schedule_lambda)\n",
        "\n",
        "img_size = torch.empty(256, 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PMMnsfN9T4h-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "66ae2c47-7903-4d82-d992-bac817894e96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint is loaded!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e67051f2b1db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             loss_cyc = (L1Norm(Generator_YX(Y_to_X), input_X) +\n\u001b[0;32m---> 50\u001b[0;31m                         L1Norm(Generator_XY(Y_to_X), input_Y)) / 2 * LAMBDA\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             loss_identity = (L1Norm(X_to_Y, input_Y) +\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-35b8c563f388>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-35b8c563f388>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    441\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    442\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 443\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "writer = SummaryWriter(LOG_DIR)\n",
        "iteration = 0\n",
        "\n",
        "ckpt_path = os.path.join(CKPT_DIR, 'lastest.pt')\n",
        "if os.path.exists(ckpt_path):\n",
        "    ckpt = torch.load(ckpt_path, map_location=torch.device(DEVICE))\n",
        "    try:\n",
        "        optimizer_discriminator_X.load_state_dict(ckpt['optim_discriminator_X'])\n",
        "        optimizer_discriminator_Y.load_state_dict(ckpt['optim_discriminator_Y'])\n",
        "        optimizer_generator.load_state_dict(ckpt['optim_generator'])\n",
        "        Discriminator_X.load_state_dict(ckpt['Discriminator_X'])\n",
        "        Discriminator_Y.load_state_dict(ckpt['Discriminator_Y'])\n",
        "        Generator_XY.load_state_dict(ckpt['Generator_XY'])\n",
        "        Generator_YX.load_state_dict(ckpt['Generator_YX'])\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print('wrong checkpoint')\n",
        "    else:\n",
        "        print('checkpoint is loaded!')\n",
        "\n",
        "\n",
        "for epoch in range(MAX_EPOCH):\n",
        "    Generator_XY.train()\n",
        "    Generator_YX.train()\n",
        "    Discriminator_X.train()\n",
        "    Discriminator_Y.train()\n",
        "    for input_X, input_Y in train_dataloader:\n",
        "        Generator_XY.train()\n",
        "        Generator_YX.train()\n",
        "        Discriminator_X.train()\n",
        "        Discriminator_Y.train()\n",
        "        for input_X, input_Y in train_dataloader:\n",
        "            iteration += 1\n",
        "            input_X = input_X.to(DEVICE)\n",
        "            input_Y = input_Y.to(DEVICE)\n",
        "\n",
        "            X_to_Y = Generator_XY(input_X).detach()\n",
        "            Y_to_X = Generator_YX(input_Y).detach()\n",
        "\n",
        "            MSELoss = torch.nn.MSELoss().to(DEVICE)\n",
        "            L1Norm = torch.nn.L1Loss().to(DEVICE)\n",
        "\n",
        "            # Train Generator\n",
        "            result_XYY = Discriminator_Y(X_to_Y)\n",
        "            result_YXX = Discriminator_X(Y_to_X)\n",
        "            loss_GAN = (MSELoss(result_XYY, torch.ones_like(result_XYY)) +\n",
        "                        MSELoss(result_YXX, torch.ones_like(result_YXX))) / 2\n",
        "\n",
        "            loss_cyc = (L1Norm(Generator_YX(Y_to_X), input_X) +\n",
        "                        L1Norm(Generator_XY(Y_to_X), input_Y)) / 2 * LAMBDA\n",
        "\n",
        "            loss_identity = (L1Norm(X_to_Y, input_Y) +\n",
        "                             L1Norm(Y_to_X, input_X)) / 2 * LAMBDA\n",
        "\n",
        "            loss_G = loss_GAN + loss_cyc + loss_identity\n",
        "\n",
        "            optimizer_generator.zero_grad()\n",
        "            loss_G.backward()\n",
        "            optimizer_generator.step()\n",
        "\n",
        "            # Train Discriminator X\n",
        "            result_XX = Discriminator_X(input_X)\n",
        "            result_YXX = Discriminator_X(Y_to_X)\n",
        "\n",
        "            loss_DX = MSELoss(result_XX, torch.ones_like(\n",
        "                result_XX)) + MSELoss(result_YXX, torch.zeros_like(result_YXX))\n",
        "\n",
        "            optimizer_discriminator_X.zero_grad()\n",
        "            loss_DX.backward()\n",
        "            optimizer_discriminator_X.step()\n",
        "\n",
        "            # Train Discriminator Y\n",
        "            result_YY = Discriminator_Y(input_Y)\n",
        "            result_XYY = Discriminator_Y(X_to_Y)\n",
        "\n",
        "            loss_DY = MSELoss(result_YY, torch.ones_like(\n",
        "                result_YY)) + MSELoss(result_XYY, torch.zeros_like(result_XYY))\n",
        "\n",
        "            optimizer_discriminator_Y.zero_grad()\n",
        "            loss_DY.backward()\n",
        "            optimizer_discriminator_Y.step()\n",
        "\n",
        "            loss = loss_G.item() + loss_DX.item() + loss_DY.item()\n",
        "\n",
        "            if iteration % LOG_ITER == 0 and writer is not None:\n",
        "                writer.add_scalar('train_loss', loss, iteration)\n",
        "                print('[epoch: {}, iteration: {}] train loss : {:4f}'.format(\n",
        "                    epoch+1, iteration, loss))\n",
        "\n",
        "                ckpt = {'Discriminator_X': Discriminator_X.state_dict(),\n",
        "                        'Discriminator_Y': Discriminator_Y.state_dict(),\n",
        "                        'Generator_XY': Generator_XY.state_dict(),\n",
        "                        'Generator_YX': Generator_YX.state_dict(),\n",
        "                        'optim_discriminator_X': optimizer_discriminator_X.state_dict(),\n",
        "                        'optim_discriminator_Y': optimizer_discriminator_Y.state_dict(),\n",
        "                        'optim_generator': optimizer_generator.state_dict()}\n",
        "                torch.save(ckpt, ckpt_path)\n",
        "            \n",
        "                shower = torch.cat(((input_X+1)/2, (X_to_Y+1)/2), 0)\n",
        "                plt.imshow(make_grid(shower.cpu(), nrow=BATCHSIZE).permute(1,2,0))\n",
        "\n",
        "                save_image(make_grid(shower.cpu(), nrow=BATCHSIZE),\n",
        "                                os.path.join(LOG_DIR, 'iteration {}.jpg'.format(iteration)))\n",
        "                plt.show()\n",
        "\n",
        "    scheduler_G.step()\n",
        "    scheduler_DX.step()\n",
        "    scheduler_DY.step()\n",
        "    print('[epoch: {}] train loss : {:4f}'.format(epoch+1, loss))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "cyclegan_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}